<!-- loioa72266660d8f4c829b93a4f2148bbf31 -->

# View Metrics for a Model

A metric provides a measure of quality or confidence for a model.



<a name="loioa72266660d8f4c829b93a4f2148bbf31__prereq_lmh_lrd_jpb"/>

## Prerequisites

You have the `scenario_artifact_viewer` or `scenario_metric_viewer` role, or you are assigned to a role collection that contains one of these roles. For more information, see [Roles and Authorizations](roles-and-authorizations-4ef8499.md).



<a name="loioa72266660d8f4c829b93a4f2148bbf31__context_x3g_1tx_4tb"/>

## Context

During a run, metrics \(standard model evaluation metrics and associated labels, tags, and custom info\) are stored and logged by the run template. You review metrics to evaluate the quality of a model generated by the run. Metric quality is affected by the input dataset and parameter values used.

> ### Note:  
> Metric data is saved to both models and their source runs. This means you can also view the same metric data for the source run. A metric resource is a collection of all tracked metrics for a run, including labels and tags. See [View the Metric Resource for a Run](view-the-metric-resource-for-a-run-d4f29aa.md). For information about the types of data stored, see [Storing Metric Data](https://help.sap.com/viewer/2d6c5984063c40a59eda62f4a9135bee/CLOUD/en-US/ab04f048da444d13bae08214c9d40e12.html).



<a name="loioa72266660d8f4c829b93a4f2148bbf31__steps_qkj_n3p_5nb"/>

## Procedure

1.  In the *Functions Explorer* app, find the model and display its details See [Investigate a Model](investigate-a-model-81dd954.md).

2.  Select the *Metrics* tab to check the metrics and values which were captured when training the model.

    Metric details are displayed, as follows:

    -   *Name*: Quality criteria. For example, accuracy or mean absolute error \(MSE\).
    -   *Value*: Indicates a level of quality, and is dependent on the *Name* \(criteria\).
    -   *Timestamp* and *Step*: Used to uniquely identify or differentiate the results. For metrics that are logged multiple times, the timestamp and step can be used in conjunction to check how a metric has progressed during the run \(training process\). For example, a model trains iteratively on the same dataset in a single training process. Also known as an epoch.
    -   *Labels*: Custom information associated with the metric for that run template.


**Related Information**  


[Compare Model Metrics](compare-model-metrics-e357639.md "You canÂ compare metrics for models to determine which configuration parameters result in optimum results.")


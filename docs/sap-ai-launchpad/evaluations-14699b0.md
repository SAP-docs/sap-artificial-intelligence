<!-- loio14699b00e7944751bbf07f148bdcb44b -->

# Evaluations

Evaluations provides tools for benchmarking large language models and prompts via orchestration configurations.

Evaluations can be used for the following use cases:

-   Evaluating prompt templates and models as orchestration configurations to determine the most effective combination for a use case.

-   Use industry-standard predefined metrics for model and prompt comparison with your use-case-specific dataset.

-   Use your own custom-defined metrics for your prompt and model evaluations.


**Evaluations Overview**

![Architecture diagram showing system components and workflow](images/Evaluations_b0b98ba.png)

-   **[Create an Evaluation](create-an-evaluation-c15182a.md "")**  

-   **[View Evaluations](view-evaluations-9bbaa26.md "")**  

-   **[View Evaluation Run Details](view-evaluation-run-details-fe6b3e9.md "")**  

-   **[View Evaluation Job Details](view-evaluation-job-details-200ccaf.md "")**  

-   **[Compare Runs](compare-runs-6dc7cb8.md "")**  


